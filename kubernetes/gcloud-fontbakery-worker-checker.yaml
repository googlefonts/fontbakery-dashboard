# eventually we'll have a cheap way to scale and wait in the same nodepool,
# but not this year.
# Also, we could use a priority queue for drag and drop jobs, to serve
# faster while a family-wide check runs. RabbitMq implements priority queues.
# https://www.rabbitmq.com/priority.html
# 110 is the upper limit for pods, so we can use it all
# For a standard machine 64CPU 240GB VM:
# 0.58 * 110 = 63.8
# cpu: 580m
# but, actually there seems no improvement in speed over 200m
# Thus, a smaller machine, but with ~ 220 GB (seems needed)
# But it's probably less.
# For a custom machine 34CPU 221GB (the smallest with this amount of RAM)
# cpu: 300m
# To actually be able to scale to ~ 2.5 k, we need to respect
# our quota of 500 CPUs, thus after subtracting our 16 CPU
# infrastructure VM we have 484 CPUs left, with 0.2 per pod
# we're at 22 CPU machines, max ram is 143 GB here,
# let's hope its enough.
# cpu: 200m
# ALWAYS CHOOSE:
# Preemtible: enabled! (much cheaper, our use case)
# Use 1 SSD, try 170 GB, if it doesn't cause disk pressure
# we should be able to scale up to ~22 VMS and ~ 2.5 k workers
# 5 VMS: 519 Pods
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: fontbakery-worker-checker-sprinter
spec:
  # only use and scale these for collection wide jobs
  # used last => 107 pods per machine, 642 with 6 machines.
  # 107 pods per machine, 749 with 7 machines.
  # replicas: 642
  replicas: 0
  template:
    metadata:
      labels:
        run: fontbakery-worker-checker-sprinter
    spec:
      # this has no node selector/determined node, so it will
      # also attach to the short lived sprinter nodes
      nodeSelector:
        cloud.google.com/gke-nodepool: checker-sprinter-pool-1
      containers:
      - name: fontbakery-worker-checker-sprinter
        image: gcr.io/fontbakery-168509/base-python:31
        env:
          - name: FONTBAKERY_WORKER_LOG_LEVEL
            value: "INFO"
          - name: FONTBAKERY_CHECKER_TICKS_TO_FLUSH
            value: "20"
          # python will put the jobs here and so harness the SSD goodnes
#          - name: TMPDIR
#            value: '/tmp/ssd/'
        workingDir: /var/python
        command: ["python3",  "-u", "fontbakery-worker-checker.py"]
        resources:
          requests:
            cpu: 200m
            memory: 1200Mi
        # not sure about the need for these!
        # maybe they help reducing the disc pressure problem. Needs testing.
        # They are not too bad here, this is a good case for SSD.
#        volumeMounts:
#        - mountPath: "/tmp/ssd/"
#          name: "tmp-ssd"
#      volumes:
#      - name: "tmp-ssd"
#        hostPath:
#          path: "/mnt/disks/ssd0"
---
# SSD one disk is 375 GB and the current quota is ???
# 15 VMS: 1619 Pods
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: fontbakery-worker-checker-sprinter-ssd
spec:
  # only use and scale these for collection wide jobs
  # 109 pods per machine, 749 with  machines.
  #  1 VM :  109
  #  7 VMs:  749
  # used last => 15 VMs: 1619
  replicas: 0
  #replicas: 0
  template:
    metadata:
      labels:
        run: fontbakery-worker-checker-sprinter-ssd
    spec:
      # this has no node selector/determined node, so it will
      # also attach to the short lived sprinter nodes
      nodeSelector:
        cloud.google.com/gke-nodepool: checker-sprinter-ssd-pool-1
      containers:
      - name: fontbakery-worker-checker-sprinter-ssd
        image: gcr.io/fontbakery-168509/base-python:31
        env:
          - name: FONTBAKERY_WORKER_LOG_LEVEL
            value: "INFO"
          - name: FONTBAKERY_CHECKER_TICKS_TO_FLUSH
            value: "20"
          # python will put the jobs here and so harness the SSD goodnes
          - name: TMPDIR
            value: '/tmp/ssd/'
        workingDir: /var/python
        command: ["python3",  "-u", "fontbakery-worker-checker.py"]
        resources:
          requests:
            cpu: 200m
            memory: 1200Mi
        # not sure about the need for these!
        # maybe they help reducing the disc pressure problem. Needs testing.
        # They are not too bad here, this is a good case for SSD.
        volumeMounts:
        - mountPath: "/tmp/ssd/"
          name: "tmp-ssd"
      volumes:
      - name: "tmp-ssd"
        hostPath:
          path: "/mnt/disks/ssd0"
---
# let these wait for drag and drop jobs in the normal cluster
# it seems like it is sized enough for that.
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: fontbakery-worker-checker
spec:
  replicas: 8
  template:
    metadata:
      labels:
        run: fontbakery-worker-checker
    spec:
      # this has no node selector/determined node, so it will
      # also attach to the short lived sprinter nodes
      nodeSelector:
        cloud.google.com/gke-nodepool: default-pool
      containers:
      - name: fontbakery-worker-checker
        image: gcr.io/fontbakery-168509/base-python:31
        env:
          - name: FONTBAKERY_WORKER_LOG_LEVEL
            value: "DEBUG"
          - name: FONTBAKERY_CHECKER_TICKS_TO_FLUSH
            value: "1"
        workingDir: /var/python
        command: ["python3",  "-u", "fontbakery-worker-checker.py"]
        resources:
          requests:
            cpu: 200m
